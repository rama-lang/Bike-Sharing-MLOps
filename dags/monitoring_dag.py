from airflow import DAG
from airflow.operators.python import PythonOperator, ShortCircuitOperator
from datetime import datetime
import pandas as pd
import json
import os
import sys
import requests
import boto3
from sqlalchemy import create_engine

# Path settings
sys.path.append('/opt/airflow')
try:
    from src.run_pipeline import run_pipeline
except ImportError:
    def run_pipeline():
        print("Pipeline import failed, but task is defined.")

# Evidently AI Imports
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset
from evidently.pipeline.column_mapping import ColumnMapping

# --- 1. S3 Helper Function (Bucket Auto-Creation) ---
def upload_to_s3(file_path, object_name):
    bucket_name = "monitoring-reports"
    s3_client = boto3.client(
        's3',
        endpoint_url='http://localstack:4566', 
        aws_access_key_id='test',
        aws_secret_access_key='test',
        region_name='us-east-1'
    )
    try:
        try:
            s3_client.head_bucket(Bucket=bucket_name)
        except:
            print(f"üì¶ Creating bucket: {bucket_name}")
            s3_client.create_bucket(Bucket=bucket_name)

        s3_client.upload_file(file_path, bucket_name, object_name)
        print(f"‚úÖ S3 Upload Success: {object_name}")
    except Exception as e:
        print(f"‚ùå S3 Upload Failed: {str(e)}")

# --- 2. Generate Monitoring Report ---
def generate_monitoring_report():
    try:
        engine = create_engine("postgresql://airflow:airflow@postgres/airflow")
        query = "SELECT * FROM predictions"
        df = pd.read_sql(query, engine)
        
        if len(df) < 5:
            print("‚ùå Not enough data for monitoring!")
            return False

        reference_df = df.sample(n=len(df)//2, random_state=42) 
        current_df = df.drop(reference_df.index)

        column_mapping = ColumnMapping()
       # column_mapping.target = 'raw_value'        
        column_mapping.prediction = 'predicted_cnt' 
        column_mapping.numerical_features = ['temp', 'atemp', 'hum', 'windspeed']
        column_mapping.categorical_features = ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']
        monitoring_report = Report(metrics=[DataDriftPreset()])
        monitoring_report.run(
            reference_data=reference_df, 
            current_data=current_df, 
            column_mapping=column_mapping
        )
        
        report_path_json = "/opt/airflow/src/monitoring_report.json"
        report_path_html = "/opt/airflow/src/monitoring_report.html"
        
        monitoring_report.save_json(report_path_json)
        monitoring_report.save_html(report_path_html)
        
        report_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        upload_to_s3(report_path_html, f"bike_report_{report_ts}.html")
        print(f"‚úÖ Report saved and shared: {report_path_html}")
        return True
    except Exception as e:
        print(f"‚ùå Error in report: {str(e)}")
        raise 

# --- 3. Smart Trigger (Check for Drift) ---
def check_for_issues(ti):
    report_path_json = "/opt/airflow/logs/monitoring_report.json"
    if not os.path.exists(report_path_json):
        return False
        
    try:
        with open(report_path_json, 'r') as f:
            data = json.load(f)
        
        drift_detected = False
        for metric in data.get('metrics', []):
            res = metric.get('result', {})
            if 'dataset_drift' in res:
                drift_detected = res['dataset_drift']

        # Status‡∞®‡∞ø XCom‡∞ï‡∞ø ‡∞™‡∞Ç‡∞™‡±Å‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞Ç
        ti.xcom_push(key='drift_status', value="DETECTED üö®" if drift_detected else "Normal ‚úÖ")
        
        # üö® ‡∞°‡±ç‡∞∞‡∞ø‡∞´‡±ç‡∞ü‡±ç ‡∞â‡∞Ç‡∞ü‡±á‡∞®‡±á 'True' ‡∞µ‡±Ü‡∞≥‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø, ‡∞Ö‡∞™‡±ç‡∞™‡±Å‡∞°‡±á ‡∞∏‡±ç‡∞≤‡∞æ‡∞ï‡±ç & ‡∞∞‡±Ä-‡∞ü‡±ç‡∞∞‡±à‡∞®‡±ç ‡∞∞‡∞®‡±ç ‡∞Ö‡∞µ‡±Å‡∞§‡∞æ‡∞Ø‡∞ø
        if drift_detected:
            print("üö® Drift found! Triggering alerts and retraining.")
            return True
        else:
            print("‚úÖ Everything is normal. Skipping retraining.")
            return False 
            
    except Exception as e:
        print(f"‚ùå Error in check: {str(e)}")
        return False

# --- 4. Slack Alert Function ---
def send_slack_manual(ti):
    webhook_url = os.getenv("SLACK_WEBHOOK")
    drift = ti.xcom_pull(task_ids='check_for_issues', key='drift_status')
    
    payload = {
        "text": (
            "üö® *MLOps Alert: Action Required!* \n\n"
            f"‚Ä¢ *Status:* {drift}\n"
            "‚Ä¢ *Reason:* ‡∞°‡±á‡∞ü‡∞æ‡∞≤‡±ã ‡∞Æ‡∞æ‡∞∞‡±ç‡∞™‡±Å‡∞≤‡±Å ‡∞µ‡∞ö‡±ç‡∞ö‡∞æ‡∞Ø‡∞ø (Drift), ‡∞Ö‡∞Ç‡∞¶‡±Å‡∞ï‡±á ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞®‡∞ø *Retrain* ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å! üõ†Ô∏è"
        )
    }
    requests.post(webhook_url, json=payload, timeout=10)

# --- 5. DAG Definition ---
with DAG(
    dag_id='model_monitoring_dag',
    start_date=datetime(2026, 1, 1),
    schedule_interval='@daily',
    catchup=False
) as dag:

    monitor_task = PythonOperator(task_id='generate_report', python_callable=generate_monitoring_report)
    check_issues_task = ShortCircuitOperator(task_id='check_for_issues', python_callable=check_for_issues)
    send_slack_task = PythonOperator(task_id='send_slack_alert', python_callable=send_slack_manual)
    retrain_task = PythonOperator(task_id='retrain_model', python_callable=run_pipeline)

    # Workflow
    monitor_task >> check_issues_task >> [send_slack_task, retrain_task]